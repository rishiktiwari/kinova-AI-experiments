{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from llama_cpp import Llama\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 25 key-value pairs and 291 tensors from /Volumes/Rishik T7_1/AI/LLMs/Phi-3-mini-4k-instruct-q4.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 32064\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 96\n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  12:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,32064]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,32064]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 32000\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 32000\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  24:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llm_load_vocab: special tokens definition check successful ( 323/32064 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32064\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 3072\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 96\n",
      "llm_load_print_meta: n_embd_head_k    = 96\n",
      "llm_load_print_meta: n_embd_head_v    = 96\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 3072\n",
      "llm_load_print_meta: n_embd_v_gqa     = 3072\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 8192\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 3.82 B\n",
      "llm_load_print_meta: model size       = 2.16 GiB (4.85 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 32000 '<|endoftext|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 32000 '<|endoftext|>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 32007 '<|end|>'\n",
      "llm_load_tensors: ggml ctx size =    0.30 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =  2157.95 MiB, ( 2158.02 / 10922.67)\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =    52.84 MiB\n",
      "llm_load_tensors:      Metal buffer size =  2157.95 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M1 Pro\n",
      "ggml_metal_init: picking default device: Apple M1 Pro\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M1 Pro\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple7  (1007)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =  1536.00 MiB, ( 3694.95 / 10922.67)\n",
      "llama_kv_cache_init:      Metal KV buffer size =  1536.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   288.02 MiB, ( 3982.97 / 10922.67)\n",
      "llama_new_context_with_model:      Metal compute buffer size =   288.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    14.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LAMMAFILE = 1 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'system') %}{{'<|system|>' + '\\n' + message['content'] + '<|end|>' + '\\n'}}{% elif (message['role'] == 'user') %}{{'<|user|>' + '\\n' + message['content'] + '<|end|>' + '\\n' + '<|assistant|>' + '\\n'}}{% elif message['role'] == 'assistant' %}{{message['content'] + '<|end|>' + '\\n'}}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.padding_token_id': '32000', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '32000', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '32', 'llama.context_length': '4096', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '10000.000000', 'llama.rope.dimension_count': '96', 'general.file_type': '15', 'llama.feed_forward_length': '8192', 'llama.block_count': '32', 'llama.embedding_length': '3072', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.name': 'LLaMA v2', 'llama.vocab_size': '32064'}\n",
      "Using gguf chat template: {{ bos_token }}{% for message in messages %}{% if (message['role'] == 'system') %}{{'<|system|>' + '\n",
      "' + message['content'] + '<|end|>' + '\n",
      "'}}{% elif (message['role'] == 'user') %}{{'<|user|>' + '\n",
      "' + message['content'] + '<|end|>' + '\n",
      "' + '<|assistant|>' + '\n",
      "'}}{% elif message['role'] == 'assistant' %}{{message['content'] + '<|end|>' + '\n",
      "'}}{% endif %}{% endfor %}\n",
      "Using chat eos_token: <|endoftext|>\n",
      "Using chat bos_token: <s>\n"
     ]
    }
   ],
   "source": [
    "llm = Llama(\n",
    "  model_path=\"/Volumes/Rishik T7_1/AI/LLMs/Phi-3-mini-4k-instruct-q4.gguf\",\n",
    "  n_ctx=4096,  # The max sequence length to use\n",
    "  n_threads=8,\n",
    "  n_gpu_layers=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   10977.71 ms\n",
      "llama_print_timings:      sample time =       1.12 ms /    10 runs   (    0.11 ms per token,  8944.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =     250.63 ms /     9 tokens (   27.85 ms per token,    35.91 tokens per second)\n",
      "llama_print_timings:        eval time =     220.48 ms /     9 runs   (   24.50 ms per token,    40.82 tokens per second)\n",
      "llama_print_timings:       total time =     489.31 ms /    18 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference time: 492.4ms\n",
      "\n",
      " action.pick(\"soldering iron\");\n"
     ]
    }
   ],
   "source": [
    "# prompt = \"What is 1+1=?\"\n",
    "# prompt = '''\n",
    "#     Your job is to extract action and objects from the given task. Respond only in JSON format.\n",
    "#     \\nDo not hello, comment, explain, or elaborate your thought processes. Your response should have two keys: actions and objects. Split the task in as many as sub-actions possible. Only mention the name of object.\n",
    "#     \\n\n",
    "#     \\nExample 1:\n",
    "#     \\nTask: Pick up banana and place on the plate.\n",
    "#     \\nYou: {\"actions\": [\"pick\", \"place\"], objects: [\"banana\", \"plate\"]}\n",
    "#     \\n\n",
    "#     \\nExample 2:\n",
    "#     \\nTask: Pick up the blue box.\n",
    "#     \\nYou: {\"actions\": [\"pick\"], objects: [\"blue box\"]}\n",
    "#     \\n\n",
    "#     \\n Question: Pick up the flower pot and place it inside the box.\n",
    "#     '''\n",
    "\n",
    "# taskQuestion = 'throw the coke cane in the garbage bin'\n",
    "# taskQuestion = 'Put the flower pot on the table'\n",
    "# taskQuestion = 'Pick up the red ball and place it on the glass'\n",
    "# taskQuestion = 'pickup the banana and dance'\n",
    "taskQuestion = 'Hold the soldering iron'\n",
    "# taskQuestion = 'Keep the apple in the tiffin box'\n",
    "# taskQuestion = 'सेब को टिफिन बॉक्स में रखें'\n",
    "# taskQuestion = 'Tartsa az almát a tiffin dobozban' # hungarian\n",
    "# taskQuestion = 'Vedd fel a piros labdát és tedd a dobozba' # hungarian\n",
    "# taskQuestion = 'Guarda la manzana en la caja tiffin.' # spanish\n",
    "# taskQuestion = 'Recoge la pelota y colócala en la caja' # spanish\n",
    "# taskQuestion = 'Conserva la mela nella scatola del tiffin' # italian\n",
    "\n",
    "prompt = '''\n",
    "    Your job is to extract actions and objects from the given task and invoke the allowed functions sequentially. Answer only with action.ignore() if no appropriate function available or you don't know what to do.\n",
    "    Do not greet, comment, explain, or elaborate your thought processes. Split the task in as many as sub-actions possible. Use semicolon to seperate the function calls.\n",
    "    Always recognise task language and translate the task and the object name to English.\n",
    "    Allowed functions are: action.pick(), action.pick_place(), action.ignore()\n",
    "    \\nExample 1:\n",
    "    Task: Pick up banana and place on the plate.\n",
    "    Answer: action.pick_place(\"banana\", \"plate\")\n",
    "    \\nExample 2:\n",
    "    Task: Pickup the blue box.\n",
    "    Answer: action.pick(\"blue box\")\n",
    "    \\nNow translate to English and answer for Task: ''' + taskQuestion\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "output = llm(\n",
    "  f\"<|user|>\\n{prompt}<|end|>\\n<|assistant|>\",\n",
    "  max_tokens=160,  # Generate up to 256 tokens\n",
    "  stop=[\"<|end|>\"],\n",
    "  echo=False,\n",
    "  temperature=0.3\n",
    ")\n",
    "end_time = time.perf_counter()\n",
    "print(\"inference time: %.1fms\\n\" % ((end_time-start_time)*1000,))\n",
    "\n",
    "print(output['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action.pick(\"red ball\"); action.place(\"box\")\n"
     ]
    }
   ],
   "source": [
    "cleanText = output['choices'][0]['text']\n",
    "newLineIdx = cleanText.find('\\n')\n",
    "if newLineIdx != -1:\n",
    "    cleanText = cleanText[:newLineIdx]\n",
    "\n",
    "cleanText = re.sub('[^A-Za-z0-9\\\"\\':\\{\\}\\[\\]\\_\\-\\,\\.\\(\\)\\;\\s]+', '', cleanText)\n",
    "cleanText = cleanText.strip(' ')\n",
    "print(cleanText)\n",
    "\n",
    "# handle \"target location\" as args in place()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['action.pick(\"red ball\")', 'action.place(\"box\")']\n"
     ]
    }
   ],
   "source": [
    "actions = cleanText.split(';')\n",
    "actions = [a.strip() for a in actions if a != '']\n",
    "print(actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tried chat_completion, but that works poorly and has slow inference."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kinovaAi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
